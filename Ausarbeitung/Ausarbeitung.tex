% Diese Zeile bitte -nicht- aendern.
\documentclass[course=eragp]{aspdoc}

\input{commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TODO: Ersetzen Sie in den folgenden Zeilen die entsprechenden -Texte- % mit den richtigen Werten.
\newcommand{\theGroup}{IhreGruppennummer} % Beispiel: 42
\newcommand{\theNumber}{IhreProjektnummer} % Beispiel: A123
% Authors, sorted by last name
\author{Lukas DÃ¶llerer \and Jonathan Hettwer \and Johannes Maier \and Tobias Schwarz \and Felix Solcher}
\date{Summer semester 2021}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Diese Zeile bitte -nicht- aendern. \title{Gruppe \theGroup{} -- Abgabe zu Aufgabe \theNumber}
\title{Static binary translation from RISC-V to x86\_64}

\begin{document}
\maketitle

\tableofcontents

\pagebreak

\section{Motivation and problem statement}

% Bullet points:
% - why to translate binaries?
%   - to use a program on another platform (e.g. source code lost / not available)
%   - (academic purposes :-))
% - How to translate binaries?
%   - DBT (dynamic binary translation): translate sequence of instructions and execute them
%   - SBT (static binary translation): translate binary to target ISA, then execute it (pro: higher
%     execution time)
% - comparable to difference between interpreted and compiled programs
% - our problem: SBT from RISC-V to x86-64
% - more academic problem, because most programs are available on x86-64

% TODO: Ahead Of Time

In order to natively execute machine code, you need a processor which runs this machine code. But there are
many different processor families with different instruction set architectures (ISA) for which programs are
developed. If you want to run a program which is written for an ISA where you don't have access to a
system running this ISA, e.g. RISC-V, then you can translate it and execute it on another system,
e.g.\ x86\_64. There are three main approaches:

\par

Firstly, you could use an emulator which ``[\ldots] interprets program instructions at
runtime.''\cite{binary_translation} The development is simple\ref{interpreter}, but it is a rather slow method.

\par

Secondly, you can use dynamic binary translation (DBT) which works like an emulator but uses caching
to store already translated instruction sequences for future use. This increases the execution speed
compared to an emulator because often used instructions don't need to be translated
twice.\cite{binary_translation}

\par

And thirdly, you can translate the full binary to an ISA which the target system is able to execute.
That way you can separate translation and execution, which means you only have to translate the
binary once for running it as often you need. This increases execution speed compared to DBT. This
approach is called static binary translation (SBT).\cite{binary_translation}

\par

%The difference between SBT and DBT is comparable to the difference between compiled and interpreted
%programs. But whilst compilers and interpreters are working on high level programming languages,
%e.g. C, C++ or Python, SBT and DBT are used to translate machine language files.

\par

Our task is to develop a static binary translator which translates RISC-V binaries to x86\_64
machine code. This field of binary translation is a rather academic issue because currently, x86\_64
is still a more widely used ISA than RISC-V.\cite{riscv_rises} This means most programs are already
available for the x86\_64 processor architecture and therefore there is a small set of
programs which need to be translated. With the current rise of RISC-V processors however, it is not
improbable that we will see a need for RISC-V to x86\_64 binary translation in the near future.

\section{Background}
\subsection{Short overview of RISC-V}

According to the RISC-V specification~\cite{rvspec}, RISC-V is an open and freely accessible ISA
which consists of ``[\ldots] a small base integer ISA [\ldots]'' and ``[\ldots] optional standard
extensions to support general purpose software development''\cite[p.~1]{rvspec}. There is also the
option to implement custom extensions. It is available in ``[b]oth 32-bit and 64-bit address space
variants for applications, operating system kernels, and hardware
implementations.''\cite[p.~1]{rvspec} As the name suggests, the RISC-V ISA describes a \emph{Reduced
    Instruction Set Computer}. This kind of microprocessor is characterized by a larger number of
registers, its load/store architecture, fixed-length instruction words and a generally limited
amount of instructions and instruction formats.\cite{RISCvCISC}

\par

Currently there are standard extensions for multiplication and division (M), for atomic instructions
(A), for floating point arithmetic (F,D,Q), for compressed instructions to reduce the code size (C)
and for control and status registers (Ziscr). More standard extensions are planned.\cite{rvspec}

\subsection{Differences between RISC-V and x86\_64}
In contrast to RISC-V, the x86\_64 ISA describes a CISC, a \emph{Complex Instruction Set Computer}.
Its processors have a limited number of registers, richer instruction sets and variable length
instructions. Because of the complex instruction functions and formats, logic is often implemented
in a mix between microcode and hard coded logic, executing instructions over a number of CPU clock
cycles.\cite{RISCvCISC}

\par

The x86\_64 ISA defines instructions which access memory and perform arithmetic or logical
operations at the same time. This is not possible with the load/store RISC-V architecture. Because
of their variable instruction lengths, x86\_64 instructions can contain immediate values which are
up to 64-bit wide.\cite[Vol.~2B~p.~4-35]{intel2017man} RISC-V base instructions are currently always
32-bit wide (16-bit for compressed instructions)\cite[p.~8]{rvspec} which makes loading big
immediates difficult. These are either combined by two or more separate immediate loading operations
or loaded relative to the current instruction pointer.\cite[p.~19]{rvspec} The latter is also often
used for performing jumps.\cite [p.~20]{rvspec}

\par

For the translation these differences are necessary knowledge to be able to generate efficient and
optimized code. For example some instructions need to be assembled or can be merged to a single
instruction in x86\_64.

\subsection{Inspiration for the intermediate representation}

When planning the project, we first evaluated the LLVM assembly language as a higher level
intermediate representation (IR) instead of developing our own. The existing language has the great advantage of
being integrated into the LLVM ecosystem with existing compiler backends and a well defined language reference. In
the end, we decided to build our own IR with less complicated features to only support our use case.
Our IR's features are inspired by the LLVM assembly language with the parallels being the static
typing, single static assignment (SSA) variables (\ref{ssa}) and machine-instruction-like operations.


\section{Approach}\label{approach_section}

We divided the translation process in three main parts: lifting, optimization and generation. We
made this division to structure the program and to simplify the implementation of optimizations.
%% --- im not 100% sure that this is right, please change it if its wrong --- %%
The parts are independent to at least theoretically save the opportunity to support another ISA as
source or target. But this isn't the focus of this project.
%% -------------------------------------- END ------------------------------- %%

\par

The lifter reads the instructions from the binary file and decodes the opcodes using
\texttt{frvdec}\cite{frvdec}. The resulting instructions are used to disassemble the RISC-V binary into basic
blocks, operations and variables in the IR. It contains the logic of
the input binary in an architecture-neutral form which generic optimization passes can be applied
on. They reduce the number of operations required without altering the program's behavior. The
generator compiles the IR program to x86\_64 assembly. To assemble an executable binary we use the
GNU assembler (AS)\cite{gnu_binutils}. We link our helper library with it using the GNU linker
(LD)\cite{gnu_binutils}. For further explanation, please refer to the following chapters.

\par

Figure \ref{program_overview} depicts a schematic of the basic translator structure. It shows the
operating parts of the translator and the used and created data objects, represented by arrows and
squares respectively.

\begin{figure}
    \centering
    \ProgramSchemeVersionOne{0.43}{13}
    \caption{
        A schematic representation of the translator's internal structure.
    }
    \label{program_overview}
\end{figure}

\par

Our implementation only supports 64-bit RISC-V ELF\footnote{Executable and Linking Format} binaries
which are statically linked and in little endian format. Another requirement is that the binary
conforms to the System V ABI\footnote{Application Binary Interface}. This defines, inter alia, the
system call IDs and the calling convention. These prerequisites are checked, using the metadata
contained in the ELF-file, before translating the file for debugging and assertion purposes.

\section{Intermediate representation}\label{IR}

As explained in the \nameref{approach_section} section, we use an IR to create a platform-independent
representation of the translated program's logic. This is not only useful to support different input
and output languages in the future, but also to be able to optimize the more abstract representation.
By parsing control flow and program structures we can run optimizations like
\nameref{dead_code_elimination}, \nameref{constant_folding}.

The IR is structured into functions which contain basic blocks. Basic blocks are blocks of code
which don't contain a control flow changing operation (CfOp). CfOps are jumps, subroutine calls,
return statements and system calls. They are used to connect basic blocks with each other. Each
basic block consists of a vector of variables. Each variable is an SSA (\ref{ssa}) variable which is
only being assigned to once.

\subsection{Single static assignment form}\label{ssa}

First described in 1988 for optimizing an intermediate program representation\cite{ssa_proposal},
the \emph{single static assignment form} (SSA) is used for creating and optimizing explicit control
flow graphs. It follows one basic rule: ``[E]ach variable is
assigned to exactly once in the program text.''\cite[p.~18]{ssa_proposal} In our IR, each variable
stores its source operation (if it is the result of an operation). This enables us to efficiently
backtrace variable origins and e.g.\ replace variables which can be evaluated statically with constant
immediates (\ref{constant_folding}) or eliminate any unused variables (\ref{dead_code_elimination}).

We introduce a \emph{memory token}, a required input for operations which read from memory and the
result of operations which write to memory. It creates a dependency between operations that store
and operations that load data. This should be used to keep the memory access operations
in their correct order when reordering operations inside basic blocks and functions. A writing
operation acts as a separator of reading operations. Because of the SSA variables used in the IR,
correct memory access order is automatically achieved as long as no variable is referenced before
its assignment. This insures a consistent memory access ordering and therefore the correctness of the
program.

\subsection{Static mappers}\label{statics}

To keep track of register contents between basic blocks, we introduced a concept called a
``Static Mapper'' or a ``static'' for short. When jumping between subroutines or labels in assembly,
the registers hold their values and act like parameters and return values of functions. This means
we have to keep track of input and output variables of basic blocks. To keep the IR platform
independent, we define a variable amount of statics which represent the original ISA's registers and
additional values which should be carried over between basic blocks.
For RISC-V, these are 31 general purpose registers, one memory token, 32 floating point registers
and one control and status flag register. The register \texttt{x0} is always zero and thus not
assigned to a static mapper.

\par

Each basic block ends with CfOps which redirect the control flow to other basic blocks. Every CfOp
stores its target block(s) together with a mapping which combines variables from their source basic
block with corresponding static mappers. This tells the \nameref{generator} which SSA variables
should be transferred between basic blocks.

\par

Variables can be the result of an operation, immediate values or bound to a static mapper. The
latter tells the generator that the variable should contain the value which was last assigned to
its bound static mapper. These kinds of variables are typically used to initialize the register file
of new basic blocks with the register content of its predecessor.

\par

Static mappers don't have a type, because they aren't variables themselves. They can't store values
and can't be used inside basic blocks. A static is just a fixed annotation for SSA variables which
makes transferring values between basic blocks easier. That's why variables assigned to static
mappers usually have the standard size and type of the static's associated register, e.g. \texttt{i64} for
general purpose registers (statics 0 to 31) and \texttt{f64} for floating point registers (statics 33
to 65). The concrete implementation of statics is not defined by IR and can thus be freely
decided by the generator.

\section{Lifter}
\subsection{General method}

% Bullet Points:
% - naming: lifter -> elevator: lift to higher code level
% - runs sequentially through the text section
% - storing variables assigned to registers in so called mapping (+ memory token)
% - for each instruction an instruction sequence in the ir is created
% - when discovering a cfop:
%   - finish basic block (adding cfop)
%   - address known:
%       - already scanned address: split basic block or set jump target
%       - not scanned: set as start of a basic block
%   - if unknown:
%       - use backtracking to find possible addresses (controllable through flag)
%       - else target = dummy, let runtime handle
% - last parts of lifting:
%   - add entry block with setup stack
%   - fixup jump targets / predecessors / successors / binary relative immediates

% executeable sections mÃ¼ssen nicht nur instruktionen enthalten, auch daten mÃ¶glich, von Neumann Daten & Instruktionen selber Speicher

The lifter creates the IR based on the RISC-V binary. This process is called lifting because the
machine code is lifted up to a more abstract representation at a higher level, the IR.

\subsubsection{Program decoding}

The ELF-file loader parses instructions from the \texttt{SHT\_PROGBITS}-type sections of the binary
which are marked with the flags
\texttt{SHF\_ALLOC} and \texttt{SHF\_EXECINSTR}. The flags mark that the section ``occupies memory during
process execution''\cite[p.~14]{elf_spec} and ``contains executable machine
instructions''\cite[p.~14]{elf_spec}. According to the ELF-file specification, only ``files used
during linking''\cite[p.~2]{elf_spec} must have a \emph{section header table} which holds section
information. In case there are no sections in the ELF file\footnote{e.g.\ memory dumps.} we use the
program loader information, stored in the \emph{program header table}. The program headers
specify parts of the underlying binary which are actually loaded to the final program.\footnote{Program header type ``PT\_LOAD''.}
We only parse instructions from the program headers which are marked as \emph{readable} and
\emph{executable}.

Due to the memory architecture of modern computers, instructions and data are mostly stored in the same
memory. This concept, which was first introduced by John von Neumann in 1945\cite{vna}, makes it
hard to statically distinguish between instructions and data. That is why we rely on the additional
information stored in the section- and program headers which identifies certain parts of the binary
as executable instructions. This however means we rely on the compiler and the programmer to only
use executable sections for storing instructions.

\subsubsection{IR generation}

After loading the data form the binary, the lifter runs sequentially through the discovered
instructions which are decoded using \texttt{frvdec}.\cite{frvdec} The resulting instructions are C-structs,
filled with the parsed content of the binary instruction code. This is a
more generic and easier processable form with direct access to involved register numbers and
immediate values. For each of the instructions, a variable amount of variables and operations is added to
the IR to replicate the original program's behavior.
% TODO: Add example for lifting an instruction here

\par

As explained in the IR chapter (\ref{IR}), our IR is subdivided into basic blocks which contain
consecutive instructions without a change in control flow. As soon as a control flow changing
instruction is discovered, it is lifted and set as the basic block's CfOp. A control flow changing
instruction can be assembled of more CfOps, e.g. RISC-V branches which consist of a conditional
jump and a jump to the next block if the branch isn't taken. The current basic block is finished and
a new one is created. If the jump address can be inferred from the instruction using
\nameref{backtracking} or if it is directly encoded, like in branch instructions, this address is used
to register the start of a new basic block.

\par

If the lifter didn't already parse the instructions at the registered virtual target
address\footnote{This is the case for forward jumps.}, the
lifter remembers that at this address a basic block must start.
Otherwise, if the lifter already parsed the instructions and didn't start a new basic block at the
jump target address, the block containing the jump address must be split up at the jump address.
This is required because it is only possible to jump to the start of a basic block.

\par

When lifting instructions, the lifter stores their original virtual address. This is used for
splitting basic blocks. All instructions with an original address lower than the split address are
sorted into the first block, the other ones into the second. Both blocks are connected using a
direct jump. Also the predecessor and successor lists as well as some operation inputs need to be
adjusted.

\par

If the jump address cannot be extracted from the instruction, the backtracking (\ref{backtracking})
is used to infer possible addresses which are processed with the same procedure as described before.
Using the command line flag ``\texttt{--full-backtracking}'' the user can control whether all possible
addresses should be found or the first one is used. If no jump address can be inferred, a dummy
basic block which indicates an unresolved jump target, is set as the target block. The jump address then
needs to be evaluated at runtime and jumps to an address where no basic block begins need to be
handled by the helper library (\ref{helper}) by interpreting the RISC-V instructions.

\par

While lifting the instructions a register file, called \emph{mapping}, keeps track of the variables
which represents the content of all registers, the memory token and the control and status registers
(csr). This is required to determine the inputs of the operations based on the input registers of
the instructions. For example, when lifting a RISC-V instruction which has the registers \texttt{x13}
and \texttt{x14} as inputs, the input variables are taken from the mapping at the corresponding
indices.

\par

At the start of a basic block the mapping is initialized
with variables from \nameref{statics}. The register with the index zero is a hard wired zero in
RISC-V and therefore not initialized. Furthermore, each read from the mapping at index zero is handled by adding an immediate
variable with value zero. Each write to the zero register is ignored, as it is specified for the \texttt{x0}
register.\cite{rvspec}

\subsubsection{Postprocessing}

After all available instructions have been lifted, a post processing step is required to connect
the basic blocks with each other. Because we lift the file by reading the instructions
consecutively, a basic block doesn't know its successors during lifting. Vice versa, a new basic
block doesn't know its full list of predecessors during lifting.
During post processing, we iterate over all basic blocks, assign them their jump target basic
blocks and adjust their predecessor and successor lists.

\par

The last step in lifting is to add an entry basic block that sets the RISC-V stack up and jumps to the
basic block at the ELF-file's entry address. This virtual address is stored in the ELF program header
\texttt{e\_entry} and specifies where the execution of the binary starts.\cite[p.~5]{elf_spec}


\subsection{Backtracking}\label{backtracking}
Although indirect jump addresses can be evaluated at runtime, we can only jump to the beginning of
detected basic blocks. This is the case, because we can optimize (\ref{dead_code_elimination}) and fold
instructions (\ref{constant_folding}) inside basic blocks without altering the control flow or
logic of the program. That's why there might often not be a one-to-one correspondence of RISC-V to x86\_64-Instructions.
Having a completely runnable program thus means detecting every basic block
with their corresponding entry points (including their exit points). Although determining all possible
indirect jump targets statically is not possible (e.g. virtual function calls),
% or not feasible (e.g. jump targets could be influenced by user input)
backtracking register contents during lifting can provide us with
an approximation of the target set.

\par

To statically determine indirect jump target addresses, we need to evaluate possible contents of
the jump's input register operand. Each variable stores its origin, being either the result of an
operation, an immediate value or from a static. This enables us to backtrack the value of
any variable in the call tree until we reached a variable which can't be backtracked or until we evaluated
all possible values.

\par

Registers keep their values between jumps and calls. We model this behavior using static mappers
which bind variables to their registers in between basic blocks. For backtracking, this means a
variable which is bound to a static mapper can have any value that the variable mapped to the same
static mapper in its basic block's predecessors has. This opens up a room of variables which
influence the jump target, growing with the amount of predecessors and operation inputs.

\par

Figure~\ref{backtracking_graph} shows an example for such a call tree. The basic block \texttt{bb24} ends
with an indirect jump, jumping to the address stored in \texttt{v37}. \texttt{ADD} and \texttt{SHL} are operations.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{pgfonlayer}{nodelayer}
            \node [style=BB] (0) at (-3.75, 0) {bb24};
            \node [style=VAR] (1) at (-2.25, 0) {v37};
            \node [style=OP] (2) at (-0.75, 0) {ADD};
            \node [style=VAR] (3) at (0.5, 1.5) {v36};
            \node [style=VAR] (4) at (0.5, -1.5) {v34};
            \node [style=OP] (5) at (2, -1.5) {SHL};
            \node [style=VAR] (6) at (3.5, 0.25) {v15};
            \node [style=VAR] (7) at (3.5, -1.5) {v22};
            \node [style=STATIC] (8) at (5, 0.25) {x14};
            \node [style=STATIC] (9) at (5, -1.5) {x21};
            \node [style=BB] (10) at (6.5, 1) {bb42};
            \node [style=BB] (11) at (6.5, -1) {bb23};
            \node [style=BB] (12) at (6.5, 2) {bb8};
            \node [style=BB] (13) at (6.5, 0) {bb17};
            \node [style=BB] (14) at (6.5, -2.25) {bb127};
            \node [style=VALUE] (15) at (2, 1.5) {-2};
            \node [style=VAR] (16) at (7.75, 2) {};
            \node [style=VAR] (18) at (7.75, 0) {};
            \node [style=VAR] (19) at (7.75, -1) {};
            \node [style=BB] (23) at (9.75, 1) {};
            \node [style=BB] (24) at (9.75, -2.25) {};
            \node [style=OP] (25) at (8.75, 0) {};
            \node [style=OP] (26) at (8.75, -1) {};
            \node [style=OP] (28) at (8.75, 2) {};
            \node [style=VAR] (29) at (7.75, 1) {};
            \node [style=VAR] (31) at (7.75, -2.25) {};
            \node [style=STATIC] (32) at (8.75, 1) {};
            \node [style=STATIC] (33) at (8.75, -2.25) {};
            \node [style=none] (34) at (9.75, 2) {};
            \node [style=none] (35) at (9.75, 1.5) {};
            \node [style=none] (36) at (9.75, 0.25) {};
            \node [style=none] (37) at (9.75, -0.25) {};
            \node [style=none] (38) at (9.75, -0.75) {};
            \node [style=none] (39) at (9.75, -1.25) {};
            \node [style=none] (40) at (9.75, 2.5) {};
        \end{pgfonlayer}
        \begin{pgfonlayer}{edgelayer}
            \draw [style=LINK] (0) to (1);
            \draw [style=LINK] (1) to (2);
            \draw [style=LINK] (2) to (3);
            \draw [style=LINK] (2) to (4);
            \draw [style=LINK] (3) to (15.center);
            \draw [style=LINK] (4) to (5);
            \draw [style=LINK] (5) to (7);
            \draw [style=LINK] (5) to (6);
            \draw [style=LINK] (6) to (8);
            \draw [style=LINK] (7) to (9);
            \draw [style=LINK] (9) to (14);
            \draw [style=LINK] (9) to (11);
            \draw [style=LINK] (8) to (13);
            \draw [style=LINK] (8) to (10);
            \draw [style=LINK] (8) to (12);
            \draw [style=LINK] (12) to (16);
            \draw [style=LINK] (13) to (18);
            \draw [style=LINK] (11) to (19);
            \draw [style=LINK] (16) to (28);
            \draw [style=LINK] (10) to (29);
            \draw [style=LINK] (29) to (32);
            \draw [style=LINK] (18) to (25);
            \draw [style=LINK] (14) to (31);
            \draw [style=LINK] (31) to (33);
            \draw [style=LINK] (33) to (24);
            \draw [style=LINK] (19) to (26);
            \draw [style=LINK] (32) to (23);
            \draw [style=DASHED] (28) to (34.center);
            \draw [style=DASHED] (28) to (35.center);
            \draw [style=DASHED] (25) to (36.center);
            \draw [style=DASHED] (25) to (37.center);
            \draw [style=DASHED] (26) to (38.center);
            \draw [style=DASHED] (26) to (39.center);
            \draw [style=DASHED] (28) to (40.center);
        \end{pgfonlayer}
    \end{tikzpicture}
    \caption{
        Exemplary variable backtracking graph.\\
        \textbf{bb}: BasicBlock; \textbf{v}: Variable; \textbf{x}: StaticMapper;
    }\label{backtracking_graph}
\end{figure}

% TODO: divide this into two parts, one lifter one generator? since there are two distinct parts to it
\subsection{Call and return optimization}
The RISC-V architecture doesn't include special operations for handling subroutines. A function call
is assembled as a jump which places the address of the next instruction into a \emph{link-}register
\texttt{x1} or \texttt{x5} (for the standard calling convention).\cite[p.~20]{rvspec} Returning from a
subroutine is done by jumping to the address which is contained in either of those registers. This
behavior is further optimized with the compressed instructions \texttt{C.JAL} and
\texttt{C.JALR}. These operations implicitly store the address of the next instruction
to the link register \texttt{x1}.[p.~105]\cite{rvspec}

\par

Subroutine calls which jump to an address in a register are so called \emph{icalls}. They act
similar to indirect jumps and are backtracked in the same way. However, target detection success for indirect calls
doesn't affect stack alignment. Even if we can't determine an indirect call target, we still perform
an x86\_64 \texttt{call} which pushes the instruction pointer onto the stack and jumps to the address
in the supplied register. This works as long as we were able to identify the jump address as a basic
block's entry point with the procedure described in the section\ \ref{backtracking}.

\subsection{Jump table detection}

As described in\ \cite{jump_table_paper}, n-conditional case statements, like the C programming
language's switch-case statement, are often implemented by compilers using jump
tables. A jump table is a pattern which enables efficient\footnote{Because the integer condition is
    used as the jump address selector, this is only efficient for switch-statements with small entry
    ranges (e.g.\ enum switching).} branching based
on a stored set of addresses which are possible jump targets. This means we need to detect jump tables to find
all possible basic block entry points.

\par

The jump table pattern always involves consists of a particular set of RISC-V instructions.
At the beginning, a bounds check is executed. A branching instruction is used to either jump to the
default branch or the code after the switch if the switch condition (an integer) is above the
highest case value.

\par

If this is not the case, the switch condition is multiplied by a factor of 4.
Using a combination of a \texttt{LUI} and an \texttt{ADDI} instruction, it is possible to load a full
32-bit constant\ \cite[p.~19]{rvspec} representing the start address of the jump table.
This address is added to the integer condition. The resulting value is the memory address
at which the desired jump address is stored. A 32-bit load is executed to load the value into a
register. An indirect jump to this register's content is the final instruction, executing the case
label jump.

\par

For the jump table detection, we just backtrack the previously described pattern for every
encountered indirect jump. If the jump turns out to be part of a jump table configuration, we can
extract the jump target addresses and use them as entry points for new basic blocks. This also works
for switch-statements which are compiled into a combination of jump tables and branch-based search
trees.

\section{Optimizations}
\subsection{Dead Code Elimination}\label{dead_code_elimination}

There are cases in compiled programs, where a value is written to a register, but never read from again.
When lifted, these register writes are found in the IR as unused variables, which can be removed.
This is accomplished by visiting the variables in reverse order, and dropping them when their reference-count is zero.
This also takes care of ``cascading'' deletes, as the reference-count for inputs of an operation is decreased once the
operation is deleted.

Another purpose of the Dead Code Elimination pass is to remove unused inputs from BasicBlocks. For this, the
reference-count does not suffice, since there might be circular control flow (like in loops). Instead, the Dead Code
Elimination pass marks each variable associated with side effects, i.e. variables used in \texttt{store}s,
\texttt{syscall}s and jumps to unknown targets like \texttt{ijump}s. This marking is then propagated through the IR,
until no more variables are reachable. Then, all unmarked variables and inputs can be deleted. This
technique can be compared to a graph search algorithm, which detects connected components containing an operation with
side effects.

\subsection{Constant Folding and Constant Propagation}\label{constant_folding}

Constant Folding and Constant Propagation are closely related optimizations, where operations with known input values
(i.e. immediate values) are computed, and immediate variables are propagated throughout BasicBlocks.
In our implementation, Constant Folding, Constant Propagation and operation simplifications are grouped into the same
pass.  As such, we'll further refer to these collectively as Constant Folding.

This optimization pass is done by visiting each operation in a BasicBlock in order, making the pass run in linear time.
Depending on the inputs of an operation, one of several actions can be performed.

In the simplest case, all inputs are immediates. The result of the operation can be computed, and the variable is
replaced with the resulting immediate value. One exception to this are binary-relative immediates---immediates which are
offset at runtime by the base address of the binary. Hence, only addition and subtraction with one binary-relative and
one non-binary-relative immediate operand are evaluable, and evaluating subtraction is only sensible if the
binary-relative immediate is the first operand.

The next case is one immediate input and one non-immediate (static or operation) input. Generally, this case allows for
simplification of identity operations, like addition with zero. If the non-immediate operand is an operation with an
immediate operand itself, the two immediates can be evaluated under the rules of associativity and commutativity
(Exceptions for binary-relative immediates still apply).

%A simple example for the simplification is the \texttt{NOP}-pseudoinstruction in RISC-V, which is usually encoded as
%\texttt{ADDI x0, x0, 0}.\cite[p.~20]{rvspec} The addition with zero does not change the value, and all occurrences of
%the result variable can be replaced with the input variable (In practice, this simple case can already be detected in
%the lifting stage).

As an example, take two consecutive variables, one being \texttt{add v0, 73} and the other \texttt{add v1, 64} (where v0
is some input and v1 is the first variable). The Constant folding pass replaces the second variable with \texttt{add v1,
    137}, and consequently might render the first variable unused.

This optimization is greatly simplified by the use of the SSA form in the IR, since variables can't be
reassigned. This means that the value of a variable is known just by looking at the declaration of the variable.

\subsection{Common subexpression elimination}

This optimization is sometimes also referred to as ``Value numbering'' \cite[p.~24]{ssa_proposal}: Each variable is
given a symbolic value depending on its inputs and operation, and variables with the same value are redundant. These
redundant variables can thus be all replaced by the first occurrence. In our implementation, a hash set is employed
(similar to how this optimization is described in the SSA proposal), where only relevant properties like the operation
type, IDs of inputs variables, or in the case of immediate variables the value itself are considered and compared.

Common subexpression elimination is mostly useful for eliminating duplicate immediates, which are often produced by
the Constant folding pass. Since input binaries are often already optimized, duplicate operations are rather uncommon.


\section{Generator}\label{generator}
The generator has two tasks.
Firstly, it needs to assemble all basic blocks which means assembling all variables with their associated operations,
preparing the inputs for the next basic block and creating the control flow operations.
Secondly, it needs to provide for a runtime environment. Namely the original binary in its memory-layouted
form and a stack for the translated binary to use as the x86\_64-Stack is reserved for the
generator. Furthermore, a resolver for ijumps
along with an interpreter which acts as a fallback if an indirect jump to an address at which no basic block starts is encountered
and architecture-specific functions like a routine to create an initial stack for the translated binary as well as a routine to translate syscalls.
Most of these are bundled into a static library that is linked to the generated assembly called the
Helper-Library.

\subsection{Naive implementation}

For assembly generation, the generator operates in two distinct modes which we refer to as the
``naive'' implementation and register allocation.

\par

The naive implementation is meant to serve as a simple codegen which is necessary to ease debugging
and to better evaluate gains from
optimizations in the IR as it very closely recreates the IR operations.

\par

It generates assembly for each basic block independently.
For each one it creates a stack frame which holds space for every variable in the basic block, iterates all operations and
retrieves its sources - either from a static or from the stack frame -, applies the operation and then saves the results in the stack frame again.
At the end of a block it iterates all control flow operations, evaluates their condition if necessary, writes out the inputs for the next basic block
from the stack frame and either calls a helper routine or directly jumps to the next block.

% TODO: example
\subsection{Register allocation}\label{reg_alloc}
To achieve good performance and code density though we need to employ a more sophisticated approach to generating assembly from the IR.
To do so we group together blocks which closely represent functions of the original binary and compile them together, allocate register and stack slots
for variables and pass them between the blocks. As such we keep often used vars in registers and prevent spilling of variables to statics at the end of each basic block.
The achieve this we first mark all blocks which are targets of call-cfops, then iterate all basic blocks, check if they are either the target of a call-cfop or have no known predecessors
and start the register allocation for the group of this block.
This will then compile the starting block while allocating registers and stack slots for the variables, evaluate the cfops and check whether the target of a cfop is part of the current group
(which in essence just means that it is not a call-target) and if so create an input mapping for the target which specify where each input variable is located or, when the block has already been compiled,
emit a sequence of instructions which move the variables to the position at which the target block expects them.
After this the handling of the current block is finished and if any of the targets of the block's cfops are part of the current group they are recursively compiled as well.

\par

The process of generating instructions for the basic block itself are also relatively simple. First, a rough (because cfops might need reshuffling, see later) estimate of the time-of-uses of each variable is created,
after which each operation in the basic block is then iterated, the inputs for it are loaded into registers if they are located in a static or on the stack frame and a destination register is chosen.
Here, if possible a seperate destination register than the input registers is used if the inputs are still needed later. Otherwise one of the input registers is simply overwritten.
When choosing a register to place a variable into, registers without a variable in them or registers with variables which aren't used anymore are preferred. If all registers contain variables which are still needed
the variable which is the farthest away from being used again (so the time of next use is the greatest) is chosen to be evicted to the stack.
A possible improvement here could be to create a preference for a register for each variable if it is needed at a fixed location (e.g. it is used as an input for an already compiled basic block)
and take it into account when choosing a register to place a variable into. Though because of time-constraints we didn't have time to implement this.

\par

After all blocks in a group are compiled and the final stack-frame size is known the control-flow operations for each block can be assembled.
The difference to the naive generator is that the inputs for the next block might not be written to statics but instead need to be placed in certain register or stack slots
while not overwriting values that are currently stored in them and might be needed later which is why the initial time-calculation might be wrong for these cases as it is hard to predict when such
conflicts might occur. The current approach taken is to place the input into statics first, then the stack slots, then the registers while storing values which may be overwritten in temporary locations.
Also, when generating the cfops and the target is a block not part of the current group the stack frame size for it might be different and a such it needs to be adjusted as well to prevent stack over- or underflows.


% TODO: Example

\subsubsection{Translation Blocks}
One problem that is encountered when employing register allocation on groups of basic blocks is that the programm cannot simply jump to them using indirect jumps anymore because the basic block might expect variables to be in registers
and not in statics and the stack frame, which is set up at the first block of a group, would be missing. To still make it possible for indirect jumps to jump to basic blocks which are register-allocated translation blocks need to be generated.
These translation blocks will set up the stack frame and move the inputs for the basic block to the registers or stack slots at which it expects them before jumping to them.
A problem with these blocks is that they need to be generated for a lot of basic blocks and take up a lot of space in the finished binary (sometimes even more than 30\% of the .text section) while also being rarely used
since most indirect jumps will end up at function starts which we recognize with relatively good accuracy and blocks after a function call which will be returned to.
So by only generating translation blocks for targets of function returns the binary size can be drastically reduced while also having a negliable impact on performance.

\subsubsection{Merging operations}
As x86 is a CISC-Architecture it offers a lot more sophisticated instructions than RISC-V or our IR. As such multiple operations in the IR can be merged into a single x86-Instruction under two conditions:
The sequence of operations must be mergeable and the intermediate results of these operations are not needed later.
Mergeable sequences include:
\begin{itemize}
    \item add,(cast),store: Can be merged to a single store with the add embedded in the memory operand and the cast expressed as the store width
    \item add,load,sign-/zero-extend: Can be merged into a single movsx/movzx with the add embedded in the memory operand
    \item and with 0x1f/0x3f,(cast),shr/shl/sar: Can be merged into a single shift instructions since they mask the count operand by themselves
\end{itemize}
Merging these operations not only reduces the binary size since fewer instructions need to be encoded but also increase performance significantly.

\subsection{Helper library}\label{helper}

Between RISC-V and x86\_64 there are also differences in the ABI exposed by linux at runtime.
The System V ABI defines the startup and system call convention to follow on each platform, and Linux defines the ABI of the system calls. % TODO: cite system v ?

\par

The calling convention for system calls is different between the ABIs, so at runtime every RISC-V system call is directed to a wrapper function in the Helper Library.
In most cases the only difference is the ID used to perform the system call, but in some cases arguments need to be modified before the system call can be performed.
We implemented all system calls required by the benchmarks.
However all system calls related to signals where stubbed to just return an error code, implementing would require more invasive system call rewriting and signal support was not necessary for running the benchmarks.

\par

% TODO: cite something that shows that system calls are slow anyway. => slow helper library doesn't matter very much
At startup linux also puts information on the x86\_64 stack, for example the program arguments and auxiliary vectors. % TODO: define what auxiliary vectors are and why it matters ?
Before executing the translated code these values need to be copied to the emulated Stack, and in some cases transformed.
The auxiliary vectors contain information on where to locate the Linux VDSO\cite{man_vdso}.
it is an ELF binary automatically linked by Linux that provides wrappers for some very common system calls to make them faster.
The application uses the auxiliary vectors to locate this ELF and dynamically links to the functions provided.
Because the ABI is very different we opted to remove the auxiliary vectors that allow the application to use the VDSO.
In the future a VDSO wrapper could be implemented to provide the same speed benefits.
% TODO: cite why the linux vdso can be used to speed up certain system calls

% bullet points:
% * why necessary (requirements): not everything can be translated ahead of time, sort of done
% * system v abi at startup
%   * ignoring the linux vdso
%   * translate arguments / environment to risc-v stack
% * translate syscalls at startup
%   * often pass through
%   * id mismatch
%   * argument layout mismatch
% * helper functions for ijump hash table lookup -> reference to other chapter
% * contains the interpreter
% * evaluate how well the requirements where reached, what is missing, problems encountered.

\subsection{Indirect jump resolution}

When encountering an indirect jump, the jump's input variable contains the jump target address in the
original RISC-V binary address space. We try to statically identify most of the indirect jump target
addresses during lifting and are thus able to start new basic blocks at such addresses.
This however means we need to translate the original binary's addresses to basic block start
addresses in the translated binary, at runtime. We also need a way to identify
indirect jump target addresses which aren't the start address of any lifted basic block.

We can only jump to the start of a (translation) basic block, because this is the only reference point where the
emulated riscv64 registers (statics) are stored at a known location.

In case we encounter such an unknown jump target address, we switch to the \nameref{interpreter} to
interpret the original RISC-V code just-in-time until the next start of a basic block is reached.

\par

Mapping the RISC-V indirect jump target addresses to translated basic blocks requires a
dictionary-like data structure.
The implementation has to have fast access times as we need to access entries with
every indirect jump and every interpreted instruction in the interpreter. It also has to be able
to identify invalid keys, to switch to the interpreter if the
jump target address is not a valid basic block start address. We developed the data structure in two
iterations:

\begin{enumerate}
    \item For the naive implementation, we store a specific 64-bit wide number for each lifted RISC-V address
          in the \texttt{.ro\_data} section. This recreates part of the original binary's address
          space with a known offset. The numbers are either 0 if no basic block with the given start address
          exists or the address of the basic block which starts with instructions from this RISC-V address.
          The latter is done by using the basic block labels, with the assembler calculating the actual
          address numbers.
          \par
          This method has the advantage of having a constant access time with barely no calculation overhead. It
          however means that we need to store additional information in our output assembly file in the size of the
          lifted instructions section of the original binary. This could be part of the problem of the
          assembler AS requiring a lot of memory for assembling the output assembly.

    \item To reduce this size overhead, we implemented a method for creating a nearly perfect hash table with a
          process called ``Hash, displace, and compress''\cite{CHD}. The process consist of a two-stage hash
          function which first sorts the items into buckets and later generates a hash function for each
          bucket. The hashed values are stored in one common hash table. The calculation starts with a load
          factor ($\alpha$) of 100\% and a bucket size of 19 elements ($\lambda$) per bucket in average. These values are lowered
          incrementally by 10\% and 2 elements respectively if no valid bucket-level hash function could be found for the
          existing constraints.
          While a lower average bucket size decreases the translation time (because hash functions have fewer
          collision constraints), a higher average bucket size decreases the translated binary's size (because
          every bucket needs to store its own hash function).
          The bucket-level hash functions only need a 16-bit index to be reconstructed and can
          thus
          be stored memory efficiently. These bucket hash function indices are stored in a separate table in
          the \texttt{.ro\_data} section, together with the hash table.

          \par

          For the hash function generation, we didn't use truly random hash functions. The authors of the ``Hash,
          displace, and compress'' perfect hashing method suggest using a ``heuristic hash
          function''\cite{CHD}, one of Bob Jenkins' first published hash functions.\cite{jenkins_hash_1} For
          our implementation, we use one of his latest hash functions which is supposed to be faster than his
          first proposals.\cite{jenkins_hash_2} Because we are only interested in hashing 64-bit addresses, we
          only need part of his hash function which generates hashes for byte arrays of variable sizes.

          \par

          The hashing algorithm is used in the generator for building the hash table and finding the required
          hash functions. It is also implemented in the \nameref{helper} for providing hashes for addresses
          which are only available during runtime.

          \par

          With the generated hash table and hash functions, every key is assigned an index in the hash table,
          no matter if it is a valid key which was used to build the hash table or not. That's why we store
          both the key and the value, which is the target basic block label in this case, in the hash table.
          For key validation, we first load the key from the calculated index and compare it to the input key.
          If they don't match, we know that no basic block exists for the supplied RISC-V address. Otherwise,
          we load the value from the table which is placed just 64-bits after the key.

          \par

          The hashing helps reduce the required storage space to the hash table size and the hash function
          indices table size. The storage size optimizations mean however that
          every access takes, although it's complexity is still constant, the additional time of calculating
          the hash table index.

\end{enumerate}

\begin{figure}
    \begin{centering}
        \begin{tikzpicture}
            \begin{axis}[
                    width=0.9\textwidth,
                    height=0.5\textwidth,
                    tick align=inside,
                    unbounded coords=jump,
                    xmin=0,
                    xmax=110,
                    ymin=-1,
                    ymax=22,
                    point meta min=90,
                    point meta max=108,
                    colorbar horizontal,
                    colorbar style={
                            xlabel={Assembly file size in $\frac{1}{100}\cdot \textit{Binary size without hashing}$},
                        },
                    colormap={reverse viridis}{
                            indices of colormap={\pgfplotscolormaplastindexof{viridis},...,0 of viridis}
                        },
                    xlabel={Load factor in percent},
                    ylabel={Average bucket size in $\frac{1}{\text{Bucket}}$},
                ]
                \addplot[
                    mark=square*,
                    only marks,
                    scatter,
                    scatter src=explicit,
                    mark size=3,
                ]
                table[meta=z] from {./tikz_src/hashing_assembly_size.txt};
            \end{axis}
        \end{tikzpicture}
        \caption{Effects of average bucket size and hash table load factor on final assembly file size.}\label{hashing_figure}
    \end{centering}
\end{figure}

In figure \ref{hashing_figure} we show the effects of the two major parameters for the used hashing
algorithm. Missing data points mean that the hash function algorithm was not able to find all valid
bucket-level hash functions within the 16-bit per-bucket storage limit.  The load factor is the fraction (in percent) of the
final hash table which is filled with
hashed entries. The average bucket size is used to calculate the number of buckets and therefore
directly effects the number of bucket-level hash functions required.

\par

The test was conducted on the translated binary of a simple zip program. Sources are located in the
\texttt{tests} project folder. The test was only performed once because the hashing algorithm and
the translator itself are both strictly deterministic.

\par

The analysis shows that
the amount of storage required for bucket-level hash functions does not have a noticeable impact on
the final assembly size. It also shows that finding all valid bucket-level hash functions is more
likely with a lower average number of elements per bucket. This leads to the conclusion that using
smaller buckets sizes can improve bucket-level hash function generation times while not noticeably
increasing the output assembly size.

\subsection{Interpreter}\label{interpreter}

If the interpreter is entered, the riscv64 regsiter values (represented by the static values) need
to be in the statics. The same also has to be done when exiting the interpreter. This is especially
important, when using the \nameref{reg_alloc} because the variable values are moved from the
registers in the statics and so the interpreter knowns where to take the values from.

\par

An emulator is the slowest method of translating a binary. Our interpreter is an emulator, but is
called interpreter. So the interpreter is pretty slow as it decodes the instructions at runtime
using frvdec~\cite{frvdec}. In order to be able to decode the instructions at runtime, the original
binary has to be stored as data in the result binary.

\par


% reference motivation -> emulator is slowest method
  % -> original binary in memory => decode instructions from address
% why necessary ? most ijumps can be resolved, but some can't be
% -> paper over gaps

It is implemented in C++ and uses frvdec~\cite{frvdec} for decoding instructions.
Every instruction reads registers:wq


loop over instructions until a point is found where it can return to an already translated binary block
% - statics at known location (requirement for entry and exit)
%   -> BB for exit has requirement
%   -> can't jump into middle of basicblock

Usually a translated BasicBlock will be found after few instructions, resulting in almost negliable slow down from entering the interperter.
However in some extreme cases where no return BasicBlock is found, pratically the entire program is emulated resulting in extreme slow downs.
This can be observed if no translation blocks are build, % TODO: ref chapter
resulting in less possible return targets.

% - in the best case the interpreter only runs for very few instructions, if at all, before a return to a translated basicblock is possible
%   - unoptimized -> slow, but ok as only fall back system
% - until it can jump back to the translated instructions (uses the ijump look up table), so the interperter is only
%   used if absolutely necessary and doesn't need to be very fast

% Explain why interpreter is necessary
% Trade-off: speed vs. simple implementation => JIT riscv64 code or self modifying code supported but not necessarily
%            very fast.
% however in pratice the interpreter is never run or only for a few instructions

% Bullet points:
% - emulate operations on register file (<-> statics)
%   => at entry all registers (statics) are at a known location
% - emulates instructions
%   => after every instruction the new register values are written back to this location
%   => at exit ....

% - can be used to translate/emulate a whole binary (-> '--interpreter-only') <- Complete emulation of rv64imacfd
The interperter fully emulates rv64imacfd, and therefor supports the same instructions as the translator.
If the ``--interpreter-only'' flag is supplied to the translator no BasicBlocks will be translated and all code will be interpreted.
This was used to verify the correctness of the interperter using the benchmark tests. % TODO: move to performance section ?

At runtime the CPUID~\cite{intel2017man} % TODO: cite ?
is checked for support of the FMA3 CPU extension.
If it is present the interpreter uses it for faster
floating point enulation, otherwise the fused multiply add instructions are assembled using separate multiplication and addition respectively subtraction instructions.

\section{Floating point handling \& problems}

\subsection{Lifting and generating}

% TODO: Move footnote content into normal text?
To support the \texttt{F} and \texttt{D}
\footnote{The Q standard extension isn't supported due to simplicity and lack of usage.}
standard extensions, the mapping as well as the amount of statics have to be extended to include the
floating point registers and the floating point control and status register (fcsr). To lift the
RISC-V floating point, some operations are reused, e.g. \emph{add}, \emph{sub}, and some floating
point specific are defined, e.g. \emph{fmul}, \emph{fsqrt}. The floating point variables have the
types \emph{f32} and \emph{f64} which represents single and double precision numbers, respectively.

\par

The \nameref{generator} uses the x86\_64 floating point instructions provided by the
\texttt{Streaming SIMD Extensions 2} (SSE2) which operates on the \texttt{xmm} registers. So SSE2 is
a requirement when translating programs using floating point arithmetics, but this isn't a problem
as SSE2 was released in 2000 and therefore almost all currently used x86\_64-based CPUs have
implemented it. But the usage of other x86\_64 extensions like SSE3, SSE4 and \texttt{Fused Multiply
    Add 3} (FMA3) have to be allowed using the optimize command line flag.

\par

The occurring floating point exceptions are not transferred to the variable representing the
\emph{fcsr} due to performance reasons. It is too slow to check after each operation which
exceptions were raised and then set the according flags. However it isn't a problem because only
very few programs depends on floating point exceptions.

\par

The floating point support can be deactivated using the ``--disable-fp'' command line flag in order
to prevent the lifter from adding the additional statics and to lift RISC-V floating point
instructions. The advantage is a decrease in used memory because less static variables have to be
stored.

\subsection{Rounding \& accuracy problems}

A known problem when dealing with different floating point implementations is that especially
rounding effects will lead to slightly different results. As we want to use the SSE2 floating point
instructions to provide an acceptable performance, some compromises have to be made.

\par

Firstly, for each RISC-V floating point instruction a rounding mode can be specified which is used
if the number cannot be expressed in the given number format. But as x86\_64 doesn't support such a
feature, it is quite hard to implement this in a fast way, because the results are already rounded
and therefore it isn't possible to round the result of each operation afterwards. And also this
would be the opposite of fast.

\par

Also a bitwise correct implementation as provided by the dynamic translator \texttt{QEMU-user} is
challenging to implement and pretty slow. So, we decided to ignore the rounding mode at lifting
every instruction except conversions between integers and floating points as well as between single
and double precision floating points. We made this exception as it makes a great difference when
rounding to integers. For example, the difference rounding \emph{25,5} towards positive infinity or
towards negative infinity, resulting in \emph{26} respectively \emph{25}, makes a huge difference.
Also the the later on used benchmarks (\ref{performance}) require to respect the rounding mode of
conversion instructions.

\par

There are two ways of implementing this specific rounding. The first one is to use the ``roundss''
instruction (respectively ``roundsd'' for double precision) and the according rounding mode as
immediate input. But these instructions are part of SSE4 which isn't implemented in every today used
x86\_64-based CPU.~\cite{intel2017man} Therefore this way is only used if SSE4 is allowed.

\par

The alternative way is to change the rounding mode in the \emph{MXCSR} register. The x86\_64 convert
instructions then round accordingly. But the disadvantage of this method is that the only way to
access the MXCSR register is using ``STMXCSR'' and ``LDMXCSR'' instructions which only operates on
memory operands and therefore is slower as the first method. Additionally this method uses 8
instructions in our implementation vs 1.~\cite{intel2017man}

\par

To depict these rounding differences we use a simple mandelbrot set
visualizer~\cite{mandelbrot_program}. The results provided in a textual form are transformed into a
picture coloring the mandelbrot set in a light grey, differences between the implementations with
yellow and the remainder in a dark grey. Figure~\ref{fig:mandelbrot_diff_native_x86} shows that even
two native compiled versions produce different results. The translator also has these accuracy
problems in a similar dimension, depicted by figure~\ref{fig:mandelbrot_diff_translator}.

\par

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{images/mandelbrot_differences/x86_diff.png}
    \caption{Differences in results of the mandelbrot set visualizer~\cite{mandelbrot_program} when
        comparing native x86\_64 to RISC-V.}
    \label{fig:mandelbrot_diff_native_x86}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{images/mandelbrot_differences/translated_diff.png}
    \caption{Differences in results of the mandelbrot set visualizer~\cite{mandelbrot_program} when
        comparing the translator to RISC-V.}
    \label{fig:mandelbrot_diff_translator}
\end{figure}

\par

A second problem can be observed when activating the usage of FMA3 and therefore allows the
translation of RISC-V fused multiply add instruction to x86\_64 equivalents rather than using
separate multiplication and addition respectively subtraction instructions. This again has an impact
on the accuracy, because the intermediate result isn't rounded~\cite{intel2017man}. But as shown by
figure~\ref{fig:mandelbrot_diff_fma3} this causes the result to be equal to the RISC-V result.

\par

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{images/mandelbrot_differences/fma_diff.png}
    \caption{Differences in results of the mandelbrot set visualizer~\cite{mandelbrot_program} when
        comparing the translator with all optimizations (especially FMA3) to RISC-V.}
    \label{fig:mandelbrot_diff_fma3}
\end{figure}

\subsection{Implementation problems \& solutions}

Some RISC-V instructions doesn't have an x86\_64 equivalent, especially unsigned conversions, sign
injection and floating point classify. This instructions are handled by sequences of x86\_64
operations.

\par

The conversion from and to unsigned integers are implemented using the signed version. But the
result are corrected using some arithmetic. The conversions to unsigned integers are handled by
setting the result to zero if the input floating point is signed as specified by the RISC-V
manual~\cite{rvspec} using bit arithmetic. The other way around is solved by using the 64bit
conversion for 32bit inputs and for 64bit inputs by handling according to the sign. If the value is
signed, the result of the signed conversion instruction needs to be adjusted by using bit arithmetic
as also used by gcc.

\par

We considered using the x86\_64 unsigned conversion instructions provided by the \texttt{Advanced
Vector Extensions 512} (AVX512), but as AVX512 is relatively new and implemented in only few CPUs we
decided to don't provide an optimization flag for using this instructions.

\par

The sign injection and floating point classify instruction are implemented by using bit arithmetic
to manipulate the sign bit and to check in which category\footnote{For example, sNaN, qNaN, negative
    infinity, negative normal number, etc.} the floating point value fits, respectively.

\section{Performance}\label{performance}
\subsection{Our solution}
\subsection{Optimierungen (Analyse)}
\subsection{QEMU}
\subsection{Dynamic translator last year}
\subsection{Native x86\_64}

\section{Summary}

\clearpage

% TODO: Fuegen Sie Ihre Quellen der Datei Ausarbeitung.bib hinzu Referenzieren Sie diese dann mit
% \cite{}. Beispiel: CR2 ist ein Register der x86-Architektur~\cite{intel2017man}.
\bibliographystyle{plain}
\bibliography{Ausarbeitung}{}

\end{document}
